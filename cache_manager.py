"""
ğŸ’¾ Ø³ÛŒØ³ØªÙ… Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Cache
âœ… TTL (Time To Live) Support
âœ… LRU (Least Recently Used) Eviction
âœ… Cache Statistics & Analytics
âœ… Cache Warming (Pre-loading)
âœ… Pattern-based Invalidation
âœ… Hit/Miss Rate Tracking
âœ… Auto Cleanup
âœ… Memory Limit Management
âœ… Cache Namespaces

Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡: Claude AI
ØªØ§Ø±ÛŒØ®: 2026-01-06 (Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡)
"""

import time
import logging
import threading
import pickle
import hashlib
from datetime import datetime, timedelta
from typing import Any, Optional, Dict, List, Tuple, Callable, Set
from dataclasses import dataclass, field, asdict
from collections import OrderedDict, defaultdict, deque
from pathlib import Path
import json

from config import (
    CACHE_ENABLED, 
    CACHE_DEFAULT_TTL, 
    CACHE_CLEANUP_INTERVAL
)

logger = logging.getLogger(__name__)


# ==================== Data Classes ====================

@dataclass
class CacheEntry:
    """ÛŒÚ© Ø¢ÛŒØªÙ… Ú©Ø´"""
    key: str
    value: Any
    created_at: float
    expires_at: Optional[float]
    hits: int = 0
    last_accessed: float = field(default_factory=time.time)
    size_bytes: int = 0
    namespace: str = "default"
    tags: Set[str] = field(default_factory=set)
    
    def is_expired(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ù‚Ø¶Ø§"""
        if self.expires_at is None:
            return False
        return time.time() > self.expires_at
    
    def touch(self):
        """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø²Ù…Ø§Ù† Ø¯Ø³ØªØ±Ø³ÛŒ"""
        self.hits += 1
        self.last_accessed = time.time()
    
    def get_age_seconds(self) -> float:
        """Ø³Ù† Ø¢ÛŒØªÙ… Ø¨Ù‡ Ø«Ø§Ù†ÛŒÙ‡"""
        return time.time() - self.created_at
    
    def get_ttl_remaining(self) -> Optional[float]:
        """Ø²Ù…Ø§Ù† Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ ØªØ§ Ø§Ù†Ù‚Ø¶Ø§"""
        if self.expires_at is None:
            return None
        remaining = self.expires_at - time.time()
        return max(0, remaining)
    
    def to_dict(self):
        return {
            'key': self.key,
            'created_at': datetime.fromtimestamp(self.created_at).isoformat(),
            'expires_at': datetime.fromtimestamp(self.expires_at).isoformat() if self.expires_at else None,
            'hits': self.hits,
            'last_accessed': datetime.fromtimestamp(self.last_accessed).isoformat(),
            'size_bytes': self.size_bytes,
            'namespace': self.namespace,
            'tags': list(self.tags),
            'age_seconds': self.get_age_seconds(),
            'ttl_remaining': self.get_ttl_remaining()
        }


@dataclass
class CacheStatistics:
    """Ø¢Ù…Ø§Ø± Ú©Ø´"""
    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    expirations: int = 0
    evictions: int = 0
    total_size_bytes: int = 0
    start_time: float = field(default_factory=time.time)
    
    def get_hit_rate(self) -> float:
        """Ù†Ø±Ø® Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø´"""
        total = self.hits + self.misses
        if total == 0:
            return 0.0
        return (self.hits / total) * 100
    
    def get_miss_rate(self) -> float:
        """Ù†Ø±Ø® miss"""
        return 100 - self.get_hit_rate()
    
    def get_uptime_seconds(self) -> float:
        """Ù…Ø¯Øª Ø²Ù…Ø§Ù† ÙØ¹Ø§Ù„ÛŒØª"""
        return time.time() - self.start_time
    
    def to_dict(self):
        return {
            'hits': self.hits,
            'misses': self.misses,
            'sets': self.sets,
            'deletes': self.deletes,
            'expirations': self.expirations,
            'evictions': self.evictions,
            'hit_rate': round(self.get_hit_rate(), 2),
            'miss_rate': round(self.get_miss_rate(), 2),
            'total_size_bytes': self.total_size_bytes,
            'total_size_mb': round(self.total_size_bytes / (1024 * 1024), 2),
            'uptime_seconds': self.get_uptime_seconds()
        }


# ==================== Enhanced Cache Manager ====================

class EnhancedCacheManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ´Ø±ÙØªÙ‡ Cache"""
    
    def __init__(self, 
                 enabled: bool = CACHE_ENABLED,
                 default_ttl: int = CACHE_DEFAULT_TTL,
                 max_size: int = 10000,
                 max_memory_mb: int = 500,
                 cleanup_interval: int = CACHE_CLEANUP_INTERVAL,
                 enable_persistence: bool = False,
                 persistence_path: str = "cache_data"):
        
        self.enabled = enabled
        self.default_ttl = default_ttl
        self.max_size = max_size
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.cleanup_interval = cleanup_interval
        self.enable_persistence = enable_persistence
        self.persistence_path = Path(persistence_path)
        
        # Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§ØµÙ„ÛŒ (OrderedDict Ø¨Ø±Ø§ÛŒ LRU)
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        
        # Ù†Ú¯Ø§Ø´Øª namespace Ø¨Ù‡ Ú©Ù„ÛŒØ¯Ù‡Ø§
        self._namespaces: Dict[str, Set[str]] = defaultdict(set)
        
        # Ù†Ú¯Ø§Ø´Øª tag Ø¨Ù‡ Ú©Ù„ÛŒØ¯Ù‡Ø§
        self._tags: Dict[str, Set[str]] = defaultdict(set)
        
        # Ø¢Ù…Ø§Ø±
        self.stats = CacheStatistics()
        
        # ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø¹Ù…Ù„ÛŒØ§Øª Ø§Ø®ÛŒØ±
        self.recent_operations: deque = deque(maxlen=100)
        
        # Lock Ø¨Ø±Ø§ÛŒ thread-safety
        self._lock = threading.RLock()
        
        # Cleanup task
        self._cleanup_task = None
        self._stop_cleanup = False
        
        # Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡ persistence
        if self.enable_persistence:
            self.persistence_path.mkdir(exist_ok=True)
        
        # Ø´Ø±ÙˆØ¹ cleanup Ø®ÙˆØ¯Ú©Ø§Ø±
        if self.enabled:
            self._start_cleanup_task()
        
        logger.info(
            f"âœ… Enhanced Cache Manager initialized "
            f"(Enabled: {enabled}, Max: {max_size}, TTL: {default_ttl}s)"
        )
    
    # ==================== Core Operations ====================
    
    def get(self, key: str, default: Any = None, 
            namespace: str = "default") -> Any:
        """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´"""
        if not self.enabled:
            return default
        
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            entry = self._cache.get(full_key)
            
            if entry is None:
                self.stats.misses += 1
                self._log_operation('miss', full_key)
                return default
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ù‚Ø¶Ø§
            if entry.is_expired():
                self._remove_entry(full_key, reason='expired')
                self.stats.misses += 1
                self.stats.expirations += 1
                self._log_operation('expired', full_key)
                return default
            
            # Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ùˆ Ø¬Ø§Ø¨Ø¬Ø§ÛŒÛŒ Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ (LRU)
            entry.touch()
            self._cache.move_to_end(full_key)
            
            self.stats.hits += 1
            self._log_operation('hit', full_key)
            
            return entry.value
    
    def set(self, key: str, value: Any, 
            ttl: Optional[int] = None,
            namespace: str = "default",
            tags: Optional[Set[str]] = None) -> bool:
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´"""
        if not self.enabled:
            return False
        
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø³Ø§ÛŒØ² ØªÙ‚Ø±ÛŒØ¨ÛŒ
            try:
                size_bytes = len(pickle.dumps(value))
            except:
                size_bytes = 0
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø­Ø§ÙØ¸Ù‡
            if size_bytes > self.max_memory_bytes:
                logger.warning(f"âš ï¸ Value too large for cache: {size_bytes} bytes")
                return False
            
            # Ø§Ú¯Ø± Ú©Ø´ Ù¾Ø± Ø§Ø³ØªØŒ evict Ú©Ù†
            while len(self._cache) >= self.max_size:
                self._evict_lru()
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙØ¶Ø§ÛŒ Ú©Ø§ÙÛŒ
            while (self.stats.total_size_bytes + size_bytes) > self.max_memory_bytes:
                if not self._evict_lru():
                    break
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ù…Ø§Ù† Ø§Ù†Ù‚Ø¶Ø§
            current_time = time.time()
            expires_at = None
            
            if ttl is None:
                ttl = self.default_ttl
            
            if ttl > 0:
                expires_at = current_time + ttl
            
            # Ø³Ø§Ø®Øª entry
            entry = CacheEntry(
                key=full_key,
                value=value,
                created_at=current_time,
                expires_at=expires_at,
                size_bytes=size_bytes,
                namespace=namespace,
                tags=tags or set()
            )
            
            # Ø­Ø°Ù entry Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
            if full_key in self._cache:
                self._remove_entry(full_key, reason='overwrite')
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù†
            self._cache[full_key] = entry
            self._namespaces[namespace].add(full_key)
            
            for tag in entry.tags:
                self._tags[tag].add(full_key)
            
            self.stats.total_size_bytes += size_bytes
            self.stats.sets += 1
            
            self._log_operation('set', full_key)
            
            # Persistence
            if self.enable_persistence:
                self._persist_entry(entry)
            
            return True
    
    def delete(self, key: str, namespace: str = "default") -> bool:
        """Ø­Ø°Ù Ø§Ø² Ú©Ø´"""
        if not self.enabled:
            return False
        
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            if full_key in self._cache:
                self._remove_entry(full_key, reason='delete')
                self.stats.deletes += 1
                self._log_operation('delete', full_key)
                return True
        
        return False
    
    def exists(self, key: str, namespace: str = "default") -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯"""
        if not self.enabled:
            return False
        
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            if full_key not in self._cache:
                return False
            
            entry = self._cache[full_key]
            if entry.is_expired():
                self._remove_entry(full_key, reason='expired')
                return False
            
            return True
    
    def clear(self, namespace: Optional[str] = None) -> int:
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ú©Ø´"""
        if not self.enabled:
            return 0
        
        with self._lock:
            if namespace is None:
                # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡
                count = len(self._cache)
                self._cache.clear()
                self._namespaces.clear()
                self._tags.clear()
                self.stats.total_size_bytes = 0
                self._log_operation('clear_all', 'all')
                logger.info(f"ğŸ§¹ Cache cleared: {count} items")
                return count
            else:
                # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ÛŒÚ© namespace
                keys_to_remove = list(self._namespaces.get(namespace, []))
                
                for key in keys_to_remove:
                    self._remove_entry(key, reason='clear_namespace')
                
                self._log_operation('clear_namespace', namespace)
                logger.info(f"ğŸ§¹ Namespace '{namespace}' cleared: {len(keys_to_remove)} items")
                return len(keys_to_remove)
    
    # ==================== Advanced Operations ====================
    
    def get_multi(self, keys: List[str], namespace: str = "default") -> Dict[str, Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ú†Ù†Ø¯ØªØ§ÛŒÛŒ"""
        result = {}
        for key in keys:
            value = self.get(key, namespace=namespace)
            if value is not None:
                result[key] = value
        return result
    
    def set_multi(self, items: Dict[str, Any], 
                  ttl: Optional[int] = None,
                  namespace: str = "default") -> int:
        """Ø°Ø®ÛŒØ±Ù‡ Ú†Ù†Ø¯ØªØ§ÛŒÛŒ"""
        success_count = 0
        for key, value in items.items():
            if self.set(key, value, ttl=ttl, namespace=namespace):
                success_count += 1
        return success_count
    
    def delete_multi(self, keys: List[str], namespace: str = "default") -> int:
        """Ø­Ø°Ù Ú†Ù†Ø¯ØªØ§ÛŒÛŒ"""
        deleted_count = 0
        for key in keys:
            if self.delete(key, namespace=namespace):
                deleted_count += 1
        return deleted_count
    
    def invalidate_by_tag(self, tag: str) -> int:
        """Ø­Ø°Ù Ø¨Ø± Ø§Ø³Ø§Ø³ tag"""
        with self._lock:
            keys_to_remove = list(self._tags.get(tag, []))
            
            for key in keys_to_remove:
                self._remove_entry(key, reason='invalidate_tag')
            
            if tag in self._tags:
                del self._tags[tag]
            
            logger.info(f"ğŸ—‘ Invalidated {len(keys_to_remove)} items with tag '{tag}'")
            return len(keys_to_remove)
    
    def invalidate_by_pattern(self, pattern: str, namespace: str = "default") -> int:
        """Ø­Ø°Ù Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯Ùˆ (wildcard)"""
        import fnmatch
        
        with self._lock:
            keys_to_check = list(self._namespaces.get(namespace, []))
            keys_to_remove = [
                key for key in keys_to_check
                if fnmatch.fnmatch(key, f"*{pattern}*")
            ]
            
            for key in keys_to_remove:
                self._remove_entry(key, reason='invalidate_pattern')
            
            logger.info(f"ğŸ—‘ Invalidated {len(keys_to_remove)} items matching '{pattern}'")
            return len(keys_to_remove)
    
    def touch(self, key: str, namespace: str = "default") -> bool:
        """Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ last_accessed Ø¨Ø¯ÙˆÙ† Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø¯Ø§Ø±"""
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            entry = self._cache.get(full_key)
            
            if entry and not entry.is_expired():
                entry.touch()
                self._cache.move_to_end(full_key)
                return True
        
        return False
    
    def extend_ttl(self, key: str, additional_seconds: int, 
                   namespace: str = "default") -> bool:
        """Ø§ÙØ²Ø§ÛŒØ´ TTL"""
        full_key = self._make_key(key, namespace)
        
        with self._lock:
            entry = self._cache.get(full_key)
            
            if entry and not entry.is_expired():
                if entry.expires_at:
                    entry.expires_at += additional_seconds
                    return True
        
        return False
    
    # ==================== Cache Warming ====================
    
    def warm(self, loader: Callable[[str], Any], 
             keys: List[str],
             namespace: str = "default",
             ttl: Optional[int] = None) -> int:
        """Ù¾ÛŒØ´â€ŒØ¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø´ (Cache Warming)"""
        logger.info(f"ğŸ”¥ Warming cache with {len(keys)} items...")
        
        success_count = 0
        
        for key in keys:
            try:
                value = loader(key)
                if self.set(key, value, ttl=ttl, namespace=namespace):
                    success_count += 1
            except Exception as e:
                logger.error(f"âŒ Failed to warm cache for key '{key}': {e}")
        
        logger.info(f"âœ… Cache warmed: {success_count}/{len(keys)} items")
        return success_count
    
    # ==================== Internal Methods ====================
    
    def _make_key(self, key: str, namespace: str) -> str:
        """Ø³Ø§Ø®Øª Ú©Ù„ÛŒØ¯ Ú©Ø§Ù…Ù„"""
        return f"{namespace}:{key}"
    
    def _remove_entry(self, key: str, reason: str = 'unknown'):
        """Ø­Ø°Ù ÛŒÚ© entry"""
        entry = self._cache.get(key)
        
        if entry:
            # Ú©Ø§Ù‡Ø´ Ø³Ø§ÛŒØ²
            self.stats.total_size_bytes -= entry.size_bytes
            
            # Ø­Ø°Ù Ø§Ø² namespace
            if key in self._namespaces[entry.namespace]:
                self._namespaces[entry.namespace].remove(key)
            
            # Ø­Ø°Ù Ø§Ø² tags
            for tag in entry.tags:
                if key in self._tags[tag]:
                    self._tags[tag].remove(key)
            
            # Ø­Ø°Ù Ø§Ø² Ú©Ø´
            del self._cache[key]
            
            # Persistence
            if self.enable_persistence:
                self._delete_persisted_entry(key)
    
    def _evict_lru(self) -> bool:
        """Ø­Ø°Ù Ú©Ù…â€ŒØ§Ø³ØªÙØ§Ø¯Ù‡â€ŒØªØ±ÛŒÙ† Ø¢ÛŒØªÙ… (LRU)"""
        if not self._cache:
            return False
        
        # Ø§ÙˆÙ„ÛŒÙ† Ø¢ÛŒØªÙ… OrderedDict = Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ†
        key, entry = self._cache.popitem(last=False)
        
        self.stats.total_size_bytes -= entry.size_bytes
        self.stats.evictions += 1
        
        # Ø­Ø°Ù Ø§Ø² namespace Ùˆ tags
        if key in self._namespaces[entry.namespace]:
            self._namespaces[entry.namespace].remove(key)
        
        for tag in entry.tags:
            if key in self._tags[tag]:
                self._tags[tag].remove(key)
        
        self._log_operation('evict', key)
        logger.debug(f"ğŸ—‘ Evicted LRU item: {key}")
        
        return True
    
    def _log_operation(self, operation: str, key: str):
        """Ø«Ø¨Øª Ø¹Ù…Ù„ÛŒØ§Øª"""
        self.recent_operations.append({
            'timestamp': time.time(),
            'operation': operation,
            'key': key
        })
    
    # ==================== Cleanup ====================
    
    def _start_cleanup_task(self):
        """Ø´Ø±ÙˆØ¹ task Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±"""
        def cleanup_loop():
            while not self._stop_cleanup:
                time.sleep(self.cleanup_interval)
                self.cleanup_expired()
        
        self._cleanup_task = threading.Thread(target=cleanup_loop, daemon=True)
        self._cleanup_task.start()
        
        logger.info(f"âœ… Auto cleanup started (interval: {self.cleanup_interval}s)")
    
    def cleanup_expired(self) -> int:
        """Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡"""
        with self._lock:
            expired_keys = [
                key for key, entry in self._cache.items()
                if entry.is_expired()
            ]
            
            for key in expired_keys:
                self._remove_entry(key, reason='expired')
                self.stats.expirations += 1
            
            if expired_keys:
                logger.info(f"ğŸ§¹ Cleaned {len(expired_keys)} expired items")
            
            return len(expired_keys)
    
    def stop(self):
        """ØªÙˆÙ‚Ù Ú©Ø´ Ù…Ù†ÛŒØ¬Ø±"""
        self._stop_cleanup = True
        
        if self._cleanup_task:
            self._cleanup_task.join(timeout=5)
        
        # Persistence
        if self.enable_persistence:
            self._save_all()
        
        logger.info("ğŸ›‘ Cache Manager stopped")
    
    # ==================== Persistence ====================
    
    def _persist_entry(self, entry: CacheEntry):
        """Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© entry Ø¯Ø± Ø¯ÛŒØ³Ú©"""
        try:
            filepath = self.persistence_path / f"{self._hash_key(entry.key)}.pkl"
            
            with open(filepath, 'wb') as f:
                pickle.dump(entry, f)
        
        except Exception as e:
            logger.error(f"âŒ Failed to persist entry {entry.key}: {e}")
    
    def _delete_persisted_entry(self, key: str):
        """Ø­Ø°Ù entry Ø§Ø² Ø¯ÛŒØ³Ú©"""
        try:
            filepath = self.persistence_path / f"{self._hash_key(key)}.pkl"
            
            if filepath.exists():
                filepath.unlink()
        
        except Exception as e:
            logger.error(f"âŒ Failed to delete persisted entry {key}: {e}")
    
    def _save_all(self):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ú©Ø´ Ø¯Ø± Ø¯ÛŒØ³Ú©"""
        logger.info("ğŸ’¾ Saving cache to disk...")
        
        with self._lock:
            for entry in self._cache.values():
                self._persist_entry(entry)
        
        logger.info("âœ… Cache saved")
    
    def _load_all(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø´ Ø§Ø² Ø¯ÛŒØ³Ú©"""
        if not self.persistence_path.exists():
            return
        
        logger.info("ğŸ“‚ Loading cache from disk...")
        
        loaded_count = 0
        
        for filepath in self.persistence_path.glob("*.pkl"):
            try:
                with open(filepath, 'rb') as f:
                    entry = pickle.load(f)
                
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†Ù‚Ø¶Ø§
                if not entry.is_expired():
                    self._cache[entry.key] = entry
                    self._namespaces[entry.namespace].add(entry.key)
                    
                    for tag in entry.tags:
                        self._tags[tag].add(entry.key)
                    
                    self.stats.total_size_bytes += entry.size_bytes
                    loaded_count += 1
                else:
                    filepath.unlink()
            
            except Exception as e:
                logger.error(f"âŒ Failed to load {filepath}: {e}")
        
        logger.info(f"âœ… Loaded {loaded_count} items from disk")
    
    def _hash_key(self, key: str) -> str:
        """Ø³Ø§Ø®Øª hash Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù… ÙØ§ÛŒÙ„"""
        return hashlib.md5(key.encode()).hexdigest()
    
    # ==================== Statistics & Monitoring ====================
    
    def get_stats(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ú©Ø§Ù…Ù„"""
        with self._lock:
            stats_dict = self.stats.to_dict()
            
            stats_dict.update({
                'cache_size': len(self._cache),
                'max_size': self.max_size,
                'utilization': round((len(self._cache) / self.max_size) * 100, 2) if self.max_size > 0 else 0,
                'namespaces_count': len(self._namespaces),
                'tags_count': len(self._tags),
                'enabled': self.enabled,
                'memory_utilization': round((self.stats.total_size_bytes / self.max_memory_bytes) * 100, 2) if self.max_memory_bytes > 0 else 0
            })
            
            return stats_dict
    
    def get_namespace_stats(self, namespace: str) -> Dict:
        """Ø¢Ù…Ø§Ø± ÛŒÚ© namespace"""
        with self._lock:
            keys = self._namespaces.get(namespace, set())
            
            total_size = sum(
                self._cache[key].size_bytes
                for key in keys
                if key in self._cache
            )
            
            total_hits = sum(
                self._cache[key].hits
                for key in keys
                if key in self._cache
            )
            
            return {
                'namespace': namespace,
                'items_count': len(keys),
                'total_size_bytes': total_size,
                'total_size_mb': round(total_size / (1024 * 1024), 2),
                'total_hits': total_hits
            }
    
    def get_top_items(self, limit: int = 10, 
                     sort_by: str = 'hits') -> List[Dict]:
        """Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯ ÛŒØ§ Ø¨Ø²Ø±Ú¯"""
        with self._lock:
            items = list(self._cache.values())
            
            if sort_by == 'hits':
                items.sort(key=lambda x: x.hits, reverse=True)
            elif sort_by == 'size':
                items.sort(key=lambda x: x.size_bytes, reverse=True)
            elif sort_by == 'age':
                items.sort(key=lambda x: x.get_age_seconds(), reverse=True)
            
            return [item.to_dict() for item in items[:limit]]
    
    def get_cache_report(self) -> str:
        """Ú¯Ø²Ø§Ø±Ø´ Ù…ØªÙ†ÛŒ Ú©Ø´"""
        stats = self.get_stats()
        top_items = self.get_top_items(5, sort_by='hits')
        
        report = "ğŸ’¾ **Ú¯Ø²Ø§Ø±Ø´ Cache**\n"
        report += "â•" * 40 + "\n\n"
        
        # ÙˆØ¶Ø¹ÛŒØª
        status = "âœ… ÙØ¹Ø§Ù„" if self.enabled else "âŒ ØºÛŒØ±ÙØ¹Ø§Ù„"
        report += f"**ÙˆØ¶Ø¹ÛŒØª:** {status}\n\n"
        
        # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
        report += "**ğŸ“Š Ø¢Ù…Ø§Ø±:**\n"
        report += f"â”œ Ø§Ù†Ø¯Ø§Ø²Ù‡: {stats['cache_size']}/{stats['max_size']}\n"
        report += f"â”œ Ø§Ø³ØªÙØ§Ø¯Ù‡: {stats['utilization']}%\n"
        report += f"â”œ Ø­Ø§ÙØ¸Ù‡: {stats['total_size_mb']} MB\n"
        report += f"â”” Namespaces: {stats['namespaces_count']}\n\n"
        
        # Hit Rate
        report += "**ğŸ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯:**\n"
        report += f"â”œ Hit Rate: {stats['hit_rate']}%\n"
        report += f"â”œ Hits: {stats['hits']}\n"
        report += f"â”œ Misses: {stats['misses']}\n"
        report += f"â”œ Sets: {stats['sets']}\n"
        report += f"â”” Evictions: {stats['evictions']}\n\n"
        
        # Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯ØªØ±ÛŒÙ†
        if top_items:
            report += "**ğŸ”¥ Ù¾Ø±Ø¨Ø§Ø²Ø¯ÛŒØ¯ØªØ±ÛŒÙ†:**\n"
            for i, item in enumerate(top_items[:3], 1):
                report += f"{i}. {item['hits']} hits - {item['key'][:30]}...\n"
        
        report += "\n" + "â•" * 40
        
        return report
    
    def export_stats(self, filepath: str = "cache_stats.json") -> bool:
        """Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ù…Ø§Ø± Ø¨Ù‡ JSON"""
        try:
            stats = self.get_stats()
            top_items = self.get_top_items(20)
            
            data = {
                'exported_at': datetime.now().isoformat(),
                'statistics': stats,
                'top_items': top_items,
                'namespaces': [
                    self.get_namespace_stats(ns)
                    for ns in self._namespaces.keys()
                ]
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Cache stats exported to: {filepath}")
            return True
        
        except Exception as e:
            logger.error(f"âŒ Export failed: {e}")
            return False


# ==================== Decorators ====================

def cached(ttl: Optional[int] = None, 
          namespace: str = "default",
          key_prefix: str = ""):
    """Decorator Ø¨Ø±Ø§ÛŒ Ú©Ø´ Ú©Ø±Ø¯Ù† Ù†ØªÛŒØ¬Ù‡ ØªØ§Ø¨Ø¹"""
    def decorator(func):
        import functools
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Ø³Ø§Ø®Øª Ú©Ù„ÛŒØ¯ Ú©Ø´
            cache_key = f"{key_prefix}{func.__name__}:{str(args)}:{str(kwargs)}"
            
            # Ø³Ø¹ÛŒ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´
            from logger import get_logger
            logger_instance = get_logger(__name__)
            
            # ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… cache_manager Ø¯Ø± context Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª
            cache_manager = kwargs.pop('_cache_manager', None)
            
            if cache_manager:
                cached_value = cache_manager.get(cache_key, namespace=namespace)
                
                if cached_value is not None:
                    logger_instance.debug(
                        f"ğŸ’¾ Cache hit: {func.__name__}",
                        handler_name=func.__name__
                    )
                    return cached_value
            
            # Ø§Ø¬Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹
            result = func(*args, **kwargs)
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
            if cache_manager:
                cache_manager.set(
                    cache_key, 
                    result, 
                    ttl=ttl,
                    namespace=namespace
                )
                logger_instance.debug(
                    f"ğŸ’¾ Cache set: {func.__name__}",
                    handler_name=func.__name__
                )
            
            return result
        
        return wrapper
    return decorator


def cache_invalidate(namespace: str = "default", 
                    pattern: Optional[str] = None,
                    tag: Optional[str] = None):
    """Decorator Ø¨Ø±Ø§ÛŒ invalidate Ú©Ø±Ø¯Ù† Ú©Ø´ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§"""
    def decorator(func):
        import functools
        
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            result = await func(*args, **kwargs)
            
            # Invalidate Ú©Ø´
            cache_manager = kwargs.get('_cache_manager')
            
            if cache_manager:
                if tag:
                    cache_manager.invalidate_by_tag(tag)
                elif pattern:
                    cache_manager.invalidate_by_pattern(pattern, namespace)
                else:
                    cache_manager.clear(namespace)
            
            return result
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            
            # Invalidate Ú©Ø´
            cache_manager = kwargs.get('_cache_manager')
            
            if cache_manager:
                if tag:
                    cache_manager.invalidate_by_tag(tag)
                elif pattern:
                    cache_manager.invalidate_by_pattern(pattern, namespace)
                else:
                    cache_manager.clear(namespace)
            
            return result
        
        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator


# ==================== Specialized Cache Managers ====================

class QueryCacheManager(EnhancedCacheManager):
    """Ú©Ø´ Ù…Ø®ØµÙˆØµ Ú©ÙˆØ¦Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    
    def __init__(self, **kwargs):
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ
        kwargs.setdefault('default_ttl', 300)  # 5 Ø¯Ù‚ÛŒÙ‚Ù‡
        kwargs.setdefault('max_size', 5000)
        kwargs.setdefault('namespace', 'queries')
        
        super().__init__(**kwargs)
        
        logger.info("âœ… Query Cache Manager initialized")
    
    def cache_query(self, query: str, params: tuple, result: Any, 
                   ttl: Optional[int] = None):
        """Ú©Ø´ Ú©Ø±Ø¯Ù† Ù†ØªÛŒØ¬Ù‡ Ú©ÙˆØ¦Ø±ÛŒ"""
        import hashlib
        
        # Ø³Ø§Ø®Øª Ú©Ù„ÛŒØ¯ ÛŒÚ©ØªØ§ Ø§Ø² Ú©ÙˆØ¦Ø±ÛŒ Ùˆ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
        query_hash = hashlib.md5(
            f"{query}:{str(params)}".encode()
        ).hexdigest()
        
        cache_key = f"query:{query_hash}"
        
        return self.set(
            cache_key, 
            result, 
            ttl=ttl,
            namespace='queries',
            tags={'database', 'query'}
        )
    
    def get_query(self, query: str, params: tuple) -> Optional[Any]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù†ØªÛŒØ¬Ù‡ Ú©ÙˆØ¦Ø±ÛŒ Ø§Ø² Ú©Ø´"""
        import hashlib
        
        query_hash = hashlib.md5(
            f"{query}:{str(params)}".encode()
        ).hexdigest()
        
        cache_key = f"query:{query_hash}"
        
        return self.get(cache_key, namespace='queries')
    
    def invalidate_table(self, table_name: str):
        """Invalidate ØªÙ…Ø§Ù… Ú©ÙˆØ¦Ø±ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© Ø¬Ø¯ÙˆÙ„"""
        return self.invalidate_by_pattern(table_name, namespace='queries')


class UserSessionCache(EnhancedCacheManager):
    """Ú©Ø´ Ù…Ø®ØµÙˆØµ session Ù‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±"""
    
    def __init__(self, **kwargs):
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ session
        kwargs.setdefault('default_ttl', 1800)  # 30 Ø¯Ù‚ÛŒÙ‚Ù‡
        kwargs.setdefault('max_size', 10000)
        
        super().__init__(**kwargs)
        
        logger.info("âœ… User Session Cache initialized")
    
    def set_user_session(self, user_id: int, data: Dict, 
                        ttl: Optional[int] = None):
        """Ø°Ø®ÛŒØ±Ù‡ session Ú©Ø§Ø±Ø¨Ø±"""
        return self.set(
            f"user:{user_id}",
            data,
            ttl=ttl,
            namespace='sessions',
            tags={f'user:{user_id}', 'session'}
        )
    
    def get_user_session(self, user_id: int) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª session Ú©Ø§Ø±Ø¨Ø±"""
        return self.get(f"user:{user_id}", namespace='sessions')
    
    def delete_user_session(self, user_id: int):
        """Ø­Ø°Ù session Ú©Ø§Ø±Ø¨Ø±"""
        return self.delete(f"user:{user_id}", namespace='sessions')
    
    def extend_user_session(self, user_id: int, seconds: int = 1800):
        """Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† session"""
        return self.extend_ttl(
            f"user:{user_id}", 
            seconds, 
            namespace='sessions'
        )


class ProductCacheManager(EnhancedCacheManager):
    """Ú©Ø´ Ù…Ø®ØµÙˆØµ Ù…Ø­ØµÙˆÙ„Ø§Øª"""
    
    def __init__(self, **kwargs):
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª
        kwargs.setdefault('default_ttl', 600)  # 10 Ø¯Ù‚ÛŒÙ‚Ù‡
        kwargs.setdefault('max_size', 3000)
        
        super().__init__(**kwargs)
        
        logger.info("âœ… Product Cache Manager initialized")
    
    def cache_product(self, product_id: int, product_data: Dict,
                     ttl: Optional[int] = None):
        """Ú©Ø´ Ú©Ø±Ø¯Ù† Ù…Ø­ØµÙˆÙ„"""
        return self.set(
            f"product:{product_id}",
            product_data,
            ttl=ttl,
            namespace='products',
            tags={'product', f'product:{product_id}'}
        )
    
    def get_product(self, product_id: int) -> Optional[Dict]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØµÙˆÙ„ Ø§Ø² Ú©Ø´"""
        return self.get(f"product:{product_id}", namespace='products')
    
    def cache_product_list(self, category: str, products: List[Dict],
                          ttl: Optional[int] = None):
        """Ú©Ø´ Ú©Ø±Ø¯Ù† Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª"""
        return self.set(
            f"products:category:{category}",
            products,
            ttl=ttl,
            namespace='products',
            tags={'product_list', f'category:{category}'}
        )
    
    def get_product_list(self, category: str) -> Optional[List[Dict]]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù…Ø­ØµÙˆÙ„Ø§Øª"""
        return self.get(
            f"products:category:{category}",
            namespace='products'
        )
    
    def invalidate_product(self, product_id: int):
        """Invalidate ÛŒÚ© Ù…Ø­ØµÙˆÙ„"""
        self.delete(f"product:{product_id}", namespace='products')
        self.invalidate_by_tag(f'product:{product_id}')
    
    def invalidate_category(self, category: str):
        """Invalidate Ù…Ø­ØµÙˆÙ„Ø§Øª ÛŒÚ© Ø¯Ø³ØªÙ‡"""
        self.invalidate_by_tag(f'category:{category}')


# ==================== Cache Factory ====================

class CacheFactory:
    """Ú©Ø§Ø±Ø®Ø§Ù†Ù‡ Ø³Ø§Ø®Øª Cache Manager"""
    
    _instances: Dict[str, EnhancedCacheManager] = {}
    _lock = threading.Lock()
    
    @classmethod
    def get_cache(cls, cache_type: str = 'default', **kwargs) -> EnhancedCacheManager:
        """Ø¯Ø±ÛŒØ§ÙØª ÛŒØ§ Ø³Ø§Ø®Øª Cache Manager"""
        
        with cls._lock:
            if cache_type in cls._instances:
                return cls._instances[cache_type]
            
            # Ø³Ø§Ø®Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
            if cache_type == 'query':
                cache = QueryCacheManager(**kwargs)
            elif cache_type == 'session':
                cache = UserSessionCache(**kwargs)
            elif cache_type == 'product':
                cache = ProductCacheManager(**kwargs)
            else:
                cache = EnhancedCacheManager(**kwargs)
            
            cls._instances[cache_type] = cache
            return cache
    
    @classmethod
    def get_all_caches(cls) -> Dict[str, EnhancedCacheManager]:
        """Ø¯Ø±ÛŒØ§ÙØª ØªÙ…Ø§Ù… Cache Manager Ù‡Ø§"""
        return cls._instances.copy()
    
    @classmethod
    def clear_all_caches(cls):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ú©Ø´â€ŒÙ‡Ø§"""
        for cache_type, cache in cls._instances.items():
            cache.clear()
            logger.info(f"ğŸ§¹ Cleared cache: {cache_type}")
    
    @classmethod
    def stop_all_caches(cls):
        """ØªÙˆÙ‚Ù ØªÙ…Ø§Ù… Cache Manager Ù‡Ø§"""
        for cache_type, cache in cls._instances.items():
            cache.stop()
            logger.info(f"ğŸ›‘ Stopped cache: {cache_type}")


# ==================== Global Cache Manager ====================

# Ù†Ù…ÙˆÙ†Ù‡ Ø³Ø±Ø§Ø³Ø±ÛŒ
_global_cache: Optional[EnhancedCacheManager] = None


def setup_cache(enabled: bool = CACHE_ENABLED,
               default_ttl: int = CACHE_DEFAULT_TTL,
               max_size: int = 10000) -> EnhancedCacheManager:
    """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Cache Manager Ø³Ø±Ø§Ø³Ø±ÛŒ"""
    global _global_cache
    
    _global_cache = EnhancedCacheManager(
        enabled=enabled,
        default_ttl=default_ttl,
        max_size=max_size
    )
    
    return _global_cache


def get_cache() -> Optional[EnhancedCacheManager]:
    """Ø¯Ø±ÛŒØ§ÙØª Cache Manager Ø³Ø±Ø§Ø³Ø±ÛŒ"""
    if _global_cache is None:
        setup_cache()
    
    return _global_cache


# ==================== Cache Strategies ====================

class CacheStrategy:
    """Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø´"""
    
    @staticmethod
    def cache_aside(cache: EnhancedCacheManager, 
                   key: str,
                   loader: Callable[[], Any],
                   ttl: Optional[int] = None,
                   namespace: str = "default") -> Any:
        """Ø§Ù„Ú¯ÙˆÛŒ Cache-Aside (Lazy Loading)"""
        
        # Ø³Ø¹ÛŒ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´
        value = cache.get(key, namespace=namespace)
        
        if value is not None:
            return value
        
        # Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†
        value = loader()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
        cache.set(key, value, ttl=ttl, namespace=namespace)
        
        return value
    
    @staticmethod
    def write_through(cache: EnhancedCacheManager,
                     key: str,
                     value: Any,
                     writer: Callable[[Any], None],
                     ttl: Optional[int] = None,
                     namespace: str = "default"):
        """Ø§Ù„Ú¯ÙˆÛŒ Write-Through (Ù†ÙˆØ´ØªÙ† Ø¯Ø± Ú©Ø´ Ùˆ Ø¯ÛŒØªØ§Ø¨ÛŒØ³)"""
        
        # Ù†ÙˆØ´ØªÙ† Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        writer(value)
        
        # Ù†ÙˆØ´ØªÙ† Ø¯Ø± Ú©Ø´
        cache.set(key, value, ttl=ttl, namespace=namespace)
    
    @staticmethod
    def write_behind(cache: EnhancedCacheManager,
                    key: str,
                    value: Any,
                    writer: Callable[[Any], None],
                    ttl: Optional[int] = None,
                    namespace: str = "default"):
        """Ø§Ù„Ú¯ÙˆÛŒ Write-Behind (ÙÙ‚Ø· Ø¯Ø± Ú©Ø´ØŒ Ø¨Ø¹Ø¯Ø§Ù‹ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³)"""
        
        # Ù†ÙˆØ´ØªÙ† ÙÙˆØ±ÛŒ Ø¯Ø± Ú©Ø´
        cache.set(key, value, ttl=ttl, namespace=namespace)
        
        # Ù†ÙˆØ´ØªÙ† async Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ (Ø¯Ø± background)
        threading.Thread(
            target=writer,
            args=(value,),
            daemon=True
        ).start()
    
    @staticmethod
    def refresh_ahead(cache: EnhancedCacheManager,
                     key: str,
                     loader: Callable[[], Any],
                     ttl: int,
                     refresh_threshold: float = 0.8,
                     namespace: str = "default") -> Any:
        """Ø§Ù„Ú¯ÙˆÛŒ Refresh-Ahead (ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´ Ø§Ø² Ø§Ù†Ù‚Ø¶Ø§)"""
        
        value = cache.get(key, namespace=namespace)
        
        if value is not None:
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ
            full_key = cache._make_key(key, namespace)
            entry = cache._cache.get(full_key)
            
            if entry:
                ttl_remaining = entry.get_ttl_remaining()
                
                if ttl_remaining and ttl_remaining < (ttl * (1 - refresh_threshold)):
                    # ØªØ§Ø²Ù‡â€ŒØ³Ø§Ø²ÛŒ async
                    def refresh():
                        new_value = loader()
                        cache.set(key, new_value, ttl=ttl, namespace=namespace)
                    
                    threading.Thread(target=refresh, daemon=True).start()
            
            return value
        
        # Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†
        value = loader()
        cache.set(key, value, ttl=ttl, namespace=namespace)
        
        return value


# ==================== Helper Functions ====================

def create_cache_manager(cache_type: str = 'default', **kwargs) -> EnhancedCacheManager:
    """Ø³Ø§Ø®Øª Cache Manager"""
    return CacheFactory.get_cache(cache_type, **kwargs)


def format_cache_size(size_bytes: int) -> str:
    """ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ² Ú©Ø´"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"


# ==================== Cache Monitoring Integration ====================

class CacheMonitor:
    """Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯ Ú©Ø´"""
    
    def __init__(self, cache: EnhancedCacheManager):
        self.cache = cache
    
    def get_health_status(self) -> Dict:
        """ÙˆØ¶Ø¹ÛŒØª Ø³Ù„Ø§Ù…Øª Ú©Ø´"""
        stats = self.cache.get_stats()
        
        status = "ok"
        issues = []
        
        # Ø¨Ø±Ø±Ø³ÛŒ Hit Rate
        if stats['hit_rate'] < 50:
            status = "warning"
            issues.append("Low cache hit rate")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡
        if stats['utilization'] > 90:
            status = "warning"
            issues.append("Cache nearly full")
        
        if stats['utilization'] >= 100:
            status = "error"
            issues.append("Cache is full")
        
        # Ø¨Ø±Ø±Ø³ÛŒ eviction
        if stats['evictions'] > stats['sets'] * 0.3:
            status = "warning"
            issues.append("High eviction rate")
        
        return {
            'status': status,
            'enabled': self.cache.enabled,
            'hit_rate': stats['hit_rate'],
            'utilization': stats['utilization'],
            'cache_size': stats['cache_size'],
            'memory_mb': stats['total_size_mb'],
            'issues': issues
        }
    
    def get_metrics_for_monitoring(self) -> Dict:
        """Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ… Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯"""
        stats = self.cache.get_stats()
        health = self.get_health_status()
        
        return {
            'cache.hit_rate': stats['hit_rate'],
            'cache.miss_rate': stats['miss_rate'],
            'cache.size': stats['cache_size'],
            'cache.utilization': stats['utilization'],
            'cache.memory_mb': stats['total_size_mb'],
            'cache.evictions': stats['evictions'],
            'cache.health_status': health['status']
        }


# ==================== Example Usage ====================

def example_usage():
    """Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡"""
    
    # 1. Ø³Ø§Ø®Øª Cache Manager
    cache = EnhancedCacheManager(
        enabled=True,
        default_ttl=300,
        max_size=1000
    )
    
    # 2. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú©Ø´
    cache.set('user:123', {'name': 'Ali', 'age': 25}, ttl=600)
    
    # 3. Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Ú©Ø´
    user = cache.get('user:123')
    print(f"User: {user}")
    
    # 4. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² namespace
    cache.set('product:1', {'name': 'Ù…Ø§Ù†ØªÙˆ'}, namespace='products')
    
    # 5. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² tags
    cache.set(
        'order:100',
        {'user_id': 123, 'total': 50000},
        tags={'user:123', 'orders'}
    )
    
    # 6. Invalidate Ø¨Ø§ tag
    cache.invalidate_by_tag('user:123')
    
    # 7. Ø¢Ù…Ø§Ø±
    stats = cache.get_stats()
    print(f"Hit Rate: {stats['hit_rate']}%")
    
    # 8. Ú¯Ø²Ø§Ø±Ø´
    print(cache.get_cache_report())
    
    # 9. ØªÙˆÙ‚Ù
    cache.stop()


if __name__ == "__main__":
    example_usage()


logger.info("âœ… Enhanced Cache Manager module loaded")